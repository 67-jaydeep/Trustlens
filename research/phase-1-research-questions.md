RESEARCH QUESTIONS LIST (MASTER)
A. PROBLEM & PHILOSOPHY

What does “trust” mean in the context of online content?

How is “trust” different from “truth” and “accuracy”?

Can content be misleading while still being factually correct?

Why is binary classification (true/false) inadequate for misinformation?

What risks arise when a system claims to “detect fake news”?

Why is explainability more important than accuracy for this problem?

What assumptions does TRUSTLENS intentionally avoid making?

B. DOMAIN & MISINFORMATION LANDSCAPE

What are the most common real-world forms of misinformation?

How do sensationalism and framing distort perception without lying?

What role does emotional language play in content virality?

How does authority laundering (unnamed experts, vague sources) work?

How does omission of context mislead readers?

How does engagement-driven content differ from informational content?

C. HUMAN PSYCHOLOGY & MANIPULATION

Which cognitive biases are most exploited by low-trust content?

Why does emotionally charged language bypass critical thinking?

Why does fluent, confident writing feel more “true” to readers?

How do urgency and fear alter user judgment?

How do repetition and simplicity influence belief formation?

D. TRUST SIGNAL DESIGN

What qualifies as a valid “trust signal”?

Why must trust signals be observable and measurable?

Why should each trust signal be independent?

Which signals are likely to be correlated with each other?

How do we prevent double-counting risk across signals?

What signals are explainable without machine learning?

Which signals are out of scope due to project constraints?

E. AI-GENERATED CONTENT RESEARCH

Why is reliable AI-generated content detection unsolved?

What are the known failure modes of AI-detection tools?

What observable patterns are associated with AI-assisted writing?

Why should TRUSTLENS avoid claiming AI authorship detection?

How can AI-likeness be framed as risk rather than fact?

F. SCORING & INTERPRETATION

What does a “trust score” actually represent?

Should trust be represented as a number, category, or explanation?

How can scores avoid giving a false sense of certainty?

How should conflicting signal results be interpreted?

How can explanations take priority over numeric scores?

G. USERS & USE CASES

Who is the primary target user of TRUSTLENS?

What decision is the user trying to make using TRUSTLENS?

How should TRUSTLENS communicate uncertainty to users?

What misuse scenarios should be anticipated?

How can TRUSTLENS remain advisory rather than authoritative?

H. LIMITATIONS & EDGE CASES

What content types are unsuitable for trust analysis?

How does satire and parody affect trust signals?

How do cultural and linguistic differences impact signal accuracy?

How does legitimate emotional journalism affect risk scoring?

How should very short or very long content be handled?

I. ETHICS & RESPONSIBILITY

Could TRUSTLENS be used for censorship or suppression?

Who owns the interpretation of the trust score?

How can bias be introduced through signal selection?

How should limitations be communicated transparently?

What ethical boundaries should TRUSTLENS clearly state?

J. SYSTEM THINKING (PRE-PHASE 2)

Why is asynchronous analysis required for TRUSTLENS?

What happens if one trust signal fails during analysis?

Should partial results be returned or discarded?

How should trust signals be versioned over time?

What guarantees (and non-guarantees) does the system provide?